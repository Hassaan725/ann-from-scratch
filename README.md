# ann-from-scratch
A simple implementation of an Artificial Neural Network (ANN) from scratch using Python and NumPy, including forward and backward propagation.
# Artificial Neural Network (ANN) from Scratch ğŸ§ 

This project provides a simple yet complete implementation of a basic **Artificial Neural Network (ANN)** using **Python** and **NumPy**, without relying on high-level libraries like TensorFlow or PyTorch.

The goal is to demonstrate how neural networks function **internally**, focusing on the core concepts of forward propagation, backpropagation, and weight updates using gradient descent.

---

## ğŸ“Œ Features

- âœ… Manual **Forward Propagation**
- âœ… **Sigmoid Activation Function**
- âœ… Loss calculation using **Mean Squared Error (MSE)**
- âœ… Manual **Backpropagation**
- âœ… Weight updates using **Gradient Descent**
- âœ… Simple input-output training loop

---

## ğŸ§  Concepts Covered

- Neurons and layers
- Activation functions
- Feedforward computation
- Error calculation
- Gradient calculation
- Backpropagation (chain rule)
- Updating weights

---

## ğŸ§ª Sample Architecture

- Input Layer: 3 features
- Hidden Layer: 4 neurons (with sigmoid)
- Output Layer: 1 neuron (with sigmoid)

---

## ğŸ–¥ï¸ Technologies Used

- Python
- NumPy
- Jupyter Notebook (for demonstration)

---

## ğŸš€ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/your-username/ann-from-scratch.git
cd ann-from-scratch

